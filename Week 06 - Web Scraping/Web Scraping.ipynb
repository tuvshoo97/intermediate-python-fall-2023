{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Web Scraping in Python\n",
    "\n",
    "Web scraping is the process of collecting and parsing raw data from the Web, and the Python community has come up with some pretty powerful web scraping tools.\n",
    "Many disciplines, such as data science, business intelligence, and investigative reporting, can benefit enormously from collecting and analyzing data from websites."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape and Parse texts from Websites\n",
    "Collecting data from websites using an automated process is known as web scraping. Some websites explicitly forbid users from scraping their data with automated tools.\n",
    "\n",
    "**Websites have two main reasons to not allow web scraping**\n",
    "1. To protect its data. For example: Google maps do not allow users to request too many results in a minute.\n",
    "2. To prevent overuse of their servers. When bots start sending many requests website's servers slow down and thus other users will have slower connection to the website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful package for web scraping that you can find in Python’s standard library is [urllib](https://docs.python.org/3/library/urllib.html), which contains tools for working with URLs.\n",
    "**urllib** is for opening and reading URLs.\n",
    "\n",
    "#### Let's look at the example and use **urllib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "page = urlopen(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract the HTML from the page:\n",
    "1. Use html's read method to return sequence of bytes\n",
    "2. Use decode method on 1st result to decode bytes to strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x7ff490dc0c40>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_by = page.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<html>\\n<head>\\n<title>Profile: Aphrodite</title>\\n</head>\\n<body bgcolor=\"yellow\">\\n<center>\\n<br><br>\\n<img src=\"/static/aphrodite.gif\" />\\n<h2>Name: Aphrodite</h2>\\n<br><br>\\nFavorite animal: Dove\\n<br><br>\\nFavorite color: Red\\n<br><br>\\nHometown: Mount Olympus\\n</center>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html_by"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title>Profile: Aphrodite</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/aphrodite.gif\" />\n",
      "<h2>Name: Aphrodite</h2>\n",
      "<br><br>\n",
      "Favorite animal: Dove\n",
      "<br><br>\n",
      "Favorite color: Red\n",
      "<br><br>\n",
      "Hometown: Mount Olympus\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = html_by.decode(\"utf-8\")\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try to get the title of the webpage\n",
    "1. We need to get the index of the **\\<title>**, and because title tags strings have been counted we need to add it to the index. \n",
    "2. Find the index of the closing **\\<title>** tag\n",
    "3. Get the title by slicing the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"<html>\\n<head>\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title>Profile: Aphrodite</title>\\n</head>\\n<body bgcolor=\"yellow\">\\n<center>\\n<br><br>\\n<img src=\"/static/aphrodite.gif\" />\\n<h2>Name: Aphrodite</h2>\\n<br><br>\\nFavorite animal: Dove\\n<br><br>\\nFavorite color: Red\\n<br><br>\\nHometown: Mount Olympus\\n</center>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html.find(\"<title>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<title>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html[14:21]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(\"<title>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_index = html.find(\"<title>\")\n",
    "start_index = title_index + len(\"<title>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "print(start_index)\n",
    "print(title_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39\n"
     ]
    }
   ],
   "source": [
    "end_index = html.find(\"</title>\")\n",
    "print(end_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Profile: Aphrodite\n"
     ]
    }
   ],
   "source": [
    "title = html[start_index:end_index]\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### It is a lot of work just to get the title of the page. In the real world, websites are much more complex and complicated. We can use find many dedicated tools for html scraping but the most powerful and popular library for Python is [**Beautiful soup**](https://www.crummy.com/software/BeautifulSoup/)\n",
    "\n",
    "Beautiful Soup is a Python library designed for quick turnaround projects like screen-scraping.\n",
    "\n",
    "**Run the command below to install**:\n",
    "```bash\n",
    "conda install beautifulsoup4\n",
    "pip install beautifulsoup4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/amarbayanaltangerel/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/amarbayanaltangerel/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from beautifulsoup4) (2.3.2.post1)\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "page = urlopen(url)\n",
    "html = page.read().decode(\"utf-8\")\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>Profile: Aphrodite</title>\n",
       "</head>\n",
       "<body bgcolor=\"yellow\">\n",
       "<center>\n",
       "<br/><br/>\n",
       "<img src=\"/static/aphrodite.gif\"/>\n",
       "<h2>Name: Aphrodite</h2>\n",
       "<br/><br/>\n",
       "Favorite animal: Dove\n",
       "<br/><br/>\n",
       "Favorite color: Red\n",
       "<br/><br/>\n",
       "Hometown: Mount Olympus\n",
       "</center>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Aphrodite'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find(\"h2\").text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Aphrodite\n"
     ]
    }
   ],
   "source": [
    "for x in soup.find_all(\"h2\"):\n",
    "    print(x.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example above does three things\n",
    "1. Opens up a page using **urlopen** from **urllib.request**\n",
    "2. Reads and decodes the page and saves as a variable\n",
    "3. Creates a BeautifulSoup object and assigns it to the soup variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup objects have a **.get_text()** method that can be used to extract all the text from the document and automatically remove any HTML tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profile: Aphrodite\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name: Aphrodite\n",
      "\n",
      "Favorite animal: Dove\n",
      "\n",
      "Favorite color: Red\n",
      "\n",
      "Hometown: Mount Olympus\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get the title of the page, you can use **.title**, and **.string** to get the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Profile: Aphrodite'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>Profile: Aphrodite</title>\n",
      "Profile: Aphrodite\n"
     ]
    }
   ],
   "source": [
    "print(soup.title)\n",
    "print(soup.title.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use **find()** to find the tags you want and get the source attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = soup.find(\"img\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<img src=\"/static/aphrodite.gif\"/>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/static/aphrodite.gif'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ANY TEXT](http://olympus.realpython.org/static/aphrodite.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise your web scraping on Unegui.mn\n",
    "1. Go to https://www.unegui.mn/avto-mashin/-avtomashin-zarna/, Use inspection tool on your browser to see the html tags and attributes.\n",
    "2. Scrape all the listing's **title** and **price**. Scrape only the first page!\n",
    "3. Save your listings as a pandas DataFrame\n",
    "Example below illustrates the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "titles = ['Toyota FJ Cruiser, 2012/2020', 'Honda Crossroad, 2009/2019']\n",
    "prices = ['62 сая', '17 сая']\n",
    "results = pd.DataFrame([titles, prices], columns=['titles', 'prices'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>prices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Toyota FJ Cruiser, 2012/2020</td>\n",
       "      <td>Honda Crossroad, 2009/2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>62 сая</td>\n",
       "      <td>17 сая</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         titles                      prices\n",
       "0  Toyota FJ Cruiser, 2012/2020  Honda Crossroad, 2009/2019\n",
       "1                        62 сая                      17 сая"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "announcement-block__price _verified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in /Users/amarbayanaltangerel/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (2.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/amarbayanaltangerel/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/amarbayanaltangerel/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/amarbayanaltangerel/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/amarbayanaltangerel/Library/jupyterlab-desktop/jlab_server/lib/python3.8/site-packages (from requests) (2023.7.22)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get('https://www.unegui.mn/kompyuter-busad/notebook/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = soup.find_all(\"div\", {\"class\": \"announcement-block__price\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_tags = soup.find_all(\"a\", {\"class\": \"announcement-block__title\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "for a_tag in a_tags:\n",
    "    actual_link = \"https://www.unegui.mn\" + a_tag['href']\n",
    "    links.append(actual_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[90], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# title = soup.find(\"h1\", {\"class\": \"title-announcement\"}).text\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m title \u001b[38;5;241m=\u001b[39m \u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mitemprop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtitle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# price = soup.find(\"div\", {\"class\": \"announcement-price__cost\"}).text\u001b[39;00m\n\u001b[1;32m      9\u001b[0m price \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mitemprop\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprice\u001b[39m\u001b[38;5;124m\"\u001b[39m})[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['title','price'])\n",
    "\n",
    "for url in links[0:10]:\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content)\n",
    "    # title = soup.find(\"h1\", {\"class\": \"title-announcement\"}).text\n",
    "    title = soup.find(\"meta\", {\"itemprop\":\"title\"})['content']\n",
    "    # price = soup.find(\"div\", {\"class\": \"announcement-price__cost\"}).text\n",
    "    price = soup.find(\"meta\", {\"itemprop\":\"price\"})['content']\n",
    "    \n",
    "    # df = df.append({'title':title,'price':price}, ignore_index=True)\n",
    "    df = pd.concat([df, pd.DataFrame([{\"title\": title, \"price\": price}])], ignore_index=True)\n",
    "    \n",
    "    # results = soup.find_all(\"div\", {\"class\": \"announcement-block__price _verified\"})\n",
    "    # for item in results:\n",
    "    #     title = item.find(\"meta\", {\"itemprop\":\"name\"})['content']\n",
    "    #     price = item.find(\"meta\", {\"itemprop\":\"price\"})['content']\n",
    "    #     df = df.append({'title':title,'price':price}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n                          \\n                            Dell inspirion 15 3511 / 11th i5 8gb 256gb ssd /\\n                          \\n                        '"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'890000.00'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].find_all(\"meta\", {\"itemprop\":\"price\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Get a list of URLs to scrape\n",
    "2. Loop through the URLs\n",
    "3. Inside that loop, loop through the listings (65 per page)\n",
    "4. Grab the data you need (title and price for 65 listings)\n",
    "5. Append it to a dataframe\n",
    "6. Go the next page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
